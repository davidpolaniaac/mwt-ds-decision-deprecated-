enabling BFGS based optimization **without** curvature calculation
Num weight bits = 20
learning rate = 10
initial_t = 1
power_t = 0.5
decay_learning_rate = 1
using l2 regularization = 1
m = 7
Allocated 72M for weights and mem
## avg. loss 	der. mag. 	d. m. cond.	 wolfe1    	wolfe2    	mix fraction	curvature 	dir. magnitude	step size 	time      
creating cache_file = train-sets/rcv1_small.dat.cache
Reading from train-sets/rcv1_small.dat
num sources = 1
 1 6.931472e-01	1.859805e-03	3.768599e+00	          	          	          	3.414409e+01	1.977478e+04	1.103734e-01	0.298
 3 4.624629e-01	1.085869e-02	1.936935e+00	 0.554592  	0.194583  	          	          	2.208660e+02	1.000000e+00	0.407     
 4 3.384483e-01	4.246155e-04	1.726574e-01	 0.520733  	0.133178  	          	          	3.415247e+01	1.000000e+00	0.545     
 5 3.184994e-01	7.930465e-05	6.343690e-02	 0.751656  	0.517330  	          	          	6.775451e+01	1.000000e+00	0.709     
 6 3.024582e-01	2.887345e-06	1.460970e-02	 0.657158  	0.328843  	          	          	3.618754e+01	1.000000e+00	0.894     
 7 2.967627e-01	4.279793e-06	3.517536e-03	 0.645592  	0.293725  	          	          	1.585137e+01	1.000000e+00	1.098     
 8 2.952703e-01	2.241736e-06	1.631613e-03	 0.527831  	0.052654  	          	          	5.802386e+00	1.000000e+00	1.334     
 9 2.950591e-01	7.095239e-07	1.546883e-03	 0.199875  	-0.608862 	          	          	7.389074e-01	1.000000e+00	1.586     

finished run
number of examples = 200000
weighted example sum = 2e+05
weighted label sum = -1.272e+04
average loss = 0.2911
best constant = -0.06361
total feature number = 15587880
10 2.948347e-01	2.803452e-07	1.223612e-04	 0.585417  	0.171988  	          	          	1.453114e-01	1.000000e+00	2.010     

