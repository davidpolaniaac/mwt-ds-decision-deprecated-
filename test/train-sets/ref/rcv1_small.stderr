enabling BFGS based optimization **without** curvature calculation
Num weight bits = 20
learning rate = 0.5
initial_t = 1
power_t = 0.5
decay_learning_rate = 1
using l2 regularization = 1
m = 7
Allocated 72M for weights and mem
## avg. loss 	der. mag. 	d. m. cond.	 wolfe1    	wolfe2    	mix fraction	curvature 	dir. magnitude	step size 	time      
creating cache_file = train-sets/rcv1_small.dat.cache
Reading from train-sets/rcv1_small.dat
num sources = 1
 1 0.69315   	0.01247   	109.37472 	          	          	          	165.25443 	1072660.62500	0.66186   	0.299
 3 24.96380  	0.09448   	58.49783  	 -0.335275 	-0.667864 	          	          	(revise x 0.5)	0.33093   	0.346
 4 6.68890   	0.07916   	21.38882  	 -0.165651 	-0.341257 	          	          	(revise x 0.5)	0.16546   	0.406
 5 2.02595   	0.05334   	9.61256   	 -0.073645 	-0.172136 	          	          	(revise x 0.5)	0.08273   	0.471
 6 0.88609   	0.02271   	4.21489   	 -0.021323 	-0.077380 	          	          	(revise x 0.5)	0.04137   	0.532
 7 0.65787   	0.00567   	2.42293   	 0.007797  	-0.022375 	          	          	(revise x 0.5)	0.02068   	0.599
 8 0.64090   	0.00109   	2.53848   	 0.023094  	0.007617  	          	          	5.59308   	1.00000   	0.790     
 9 0.59157   	0.00084   	1.98472   	 0.939813  	0.880475  	          	          	449.76346 	1.00000   	0.989     
10 0.36198   	0.00015   	0.22195   	 0.590336  	0.255459  	          	          	123.06654 	1.00000   	1.218     
11 0.34817   	0.00506   	0.68002   	 0.218971  	-0.427074 	          	          	27.43073  	1.00000   	1.483     
12 0.32328   	0.00029   	0.08338   	 0.597310  	0.217821  	          	          	3.59897   	1.00000   	1.806     
13 0.31983   	0.00001   	0.04912   	 0.738427  	0.480963  	          	          	8.91283   	1.00000   	2.140     
14 0.31597   	0.00006   	0.04996   	 0.762547  	0.528463  	          	          	24.29690  	1.00000   	2.503     
15 0.31083   	0.00018   	0.05458   	 0.726800  	0.455733  	          	          	103.02335 	1.00000   	2.892     
16 0.29800   	0.00004   	0.01069   	 0.692399  	0.384369  	          	          	50.95579  	1.00000   	3.289     
17 0.29486   	0.00000   	0.00018   	 0.516694  	0.031776  	          	          	0.39481   	1.00000   	3.807     
18 0.29479   	0.00000   	0.00009   	 0.595978  	0.192369  	          	          	0.24621   	1.00000   	4.210     

finished run
number of examples = 200000
weighted example sum = 2e+05
weighted label sum = -1.272e+04
average loss = 0.4485
best constant = -0.06361
total feature number = 15587880
19 0.29476   	0.00000   	0.00003   	 0.619528  	0.238282  	          	          	0.15641   	1.00000   	4.608     

