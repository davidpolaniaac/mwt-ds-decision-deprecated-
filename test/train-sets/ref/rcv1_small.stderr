enabling BFGS based optimization **without** curvature calculation
Num weight bits = 20
learning rate = 10
initial_t = 1
power_t = 0.5
decay_learning_rate = 1
using l2 regularization
m = 7
Allocated 72M for weights and mem
## avg. loss 	der. mag. 	d. m. cond.	 wolfe1    	wolfe2    	mix fraction	curvature 	dir. magnitude	step size 	time      
creating cache_file = train-sets/rcv1_small.dat.cache
Reading from train-sets/rcv1_small.dat
num sources = 1
 1 6.931472e-01	1.859805e-03	3.768599e+00	          	          	          	3.414409e+01	1.977478e+04	1.103734e-01	0.225
 3 4.624629e-01	1.085869e-02	1.936935e+00	 0.554592  	0.194583  	          	          	2.208660e+02	1.000000e+00	0.301     
 4 3.384483e-01	4.246153e-04	1.726574e-01	 0.520733  	0.133178  	          	          	3.415247e+01	1.000000e+00	0.397     
 5 3.184994e-01	7.930468e-05	6.343691e-02	 0.751656  	0.517330  	          	          	6.775453e+01	1.000000e+00	0.508     
 6 3.024582e-01	2.887344e-06	1.460970e-02	 0.657158  	0.328843  	          	          	3.618753e+01	1.000000e+00	0.636     
 7 2.967627e-01	4.279800e-06	3.517536e-03	 0.645592  	0.293725  	          	          	1.585137e+01	1.000000e+00	0.779     
 8 2.952703e-01	2.241742e-06	1.631616e-03	 0.527831  	0.052654  	          	          	5.802384e+00	1.000000e+00	0.941     
 9 2.950591e-01	7.095230e-07	1.546882e-03	 0.199875  	-0.608861 	          	          	7.389067e-01	1.000000e+00	1.117     

finished run
number of examples = 200000
weighted example sum = 2e+05
weighted label sum = -1.272e+04
average loss = 0.2911
best constant = -0.06361
total feature number = 15587880
10 2.948347e-01	2.803388e-07	1.223610e-04	 0.585414  	0.171988  	          	          	1.453116e-01	1.000000e+00	1.463     

