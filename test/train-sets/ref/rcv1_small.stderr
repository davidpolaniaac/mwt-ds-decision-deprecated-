enabling BFGS based optimization **without** curvature calculation
Num weight bits = 20
learning rate = 10
initial_t = 1
power_t = 0.5
decay_learning_rate = 1
using l2 regularization
m = 7
Allocated 72M for weights and mem
## avg. loss 	der. mag. 	d. m. cond.	 wolfe1    	wolfe2    	mix fraction	curvature 	dir. magnitude	step size 	time      
creating cache_file = train-sets/rcv1_small.dat.cache
Reading from train-sets/rcv1_small.dat
num sources = 1
 1 6.931472e-01	1.859805e-03	3.768599e+00	          	          	          	3.414409e+01	1.977478e+04	1.103734e-01	0.626
 3 4.624629e-01	1.085868e-02	1.936935e+00	 0.554592  	0.194583  	          	          	2.208660e+02	1.000000e+00	0.907     
 4 3.384483e-01	4.246153e-04	1.726574e-01	 0.520733  	0.133178  	          	          	3.415247e+01	1.000000e+00	1.279     
 5 3.184994e-01	7.930471e-05	6.343691e-02	 0.751656  	0.517330  	          	          	6.775453e+01	1.000000e+00	1.659     
 6 3.024582e-01	2.887344e-06	1.460970e-02	 0.657158  	0.328843  	          	          	3.618753e+01	1.000000e+00	2.137     
 7 2.967627e-01	4.279800e-06	3.517537e-03	 0.645592  	0.293725  	          	          	1.585137e+01	1.000000e+00	2.614     
 8 2.952703e-01	2.241742e-06	1.631613e-03	 0.527831  	0.052654  	          	          	5.802388e+00	1.000000e+00	3.155     
 9 2.950591e-01	7.095246e-07	1.546885e-03	 0.199874  	-0.608862 	          	          	7.389079e-01	1.000000e+00	3.743     

finished run
number of examples = 200000
weighted example sum = 2e+05
weighted label sum = -1.272e+04
average loss = 0.2911
best constant = -0.06361
total feature number = 15587880
10 2.948347e-01	2.803461e-07	1.223612e-04	 0.585417  	0.171988  	          	          	1.453114e-01	1.000000e+00	5.077     

