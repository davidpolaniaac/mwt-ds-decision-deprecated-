enabling BFGS based optimization **without** curvature calculation
Num weight bits = 20
learning rate = 10
initial_t = 1
power_t = 0.5
decay_learning_rate = 1
using l2 regularization
m = 7
Allocated 72M for weights and mem
## avg. loss 	der. mag. 	d. m. cond.	 wolfe1    	wolfe2    	mix fraction	curvature 	dir. magnitude	step size 	time      
creating cache_file = train-sets/rcv1_small.dat.cache
Reading from train-sets/rcv1_small.dat
num sources = 1
 1 6.931472e-01	1.859805e-03	3.768599e+00	          	          	          	3.414409e+01	1.977478e+04	1.103734e-01	0.243
 3 4.624629e-01	1.085869e-02	1.936935e+00	 0.554592  	0.194583  	          	          	2.208660e+02	1.000000e+00	0.321     
 4 3.384483e-01	4.246153e-04	1.726574e-01	 0.520733  	0.133178  	          	          	3.415247e+01	1.000000e+00	0.421     
 5 3.184994e-01	7.930473e-05	6.343691e-02	 0.751656  	0.517330  	          	          	6.775453e+01	1.000000e+00	0.543     
 6 3.024582e-01	2.887343e-06	1.460970e-02	 0.657158  	0.328843  	          	          	3.618753e+01	1.000000e+00	0.677     
 7 2.967627e-01	4.279802e-06	3.517536e-03	 0.645592  	0.293725  	          	          	1.585137e+01	1.000000e+00	0.830     
 8 2.952703e-01	2.241742e-06	1.631616e-03	 0.527831  	0.052654  	          	          	5.802382e+00	1.000000e+00	1.000     
 9 2.950591e-01	7.095233e-07	1.546881e-03	 0.199875  	-0.608861 	          	          	7.389063e-01	1.000000e+00	1.187     

finished run
number of examples = 200000
weighted example sum = 2e+05
weighted label sum = -1.272e+04
average loss = 0.2911
best constant = -0.06361
total feature number = 15587880
10 2.948347e-01	2.803406e-07	1.223611e-04	 0.585413  	0.171988  	          	          	1.453117e-01	1.000000e+00	1.530     

