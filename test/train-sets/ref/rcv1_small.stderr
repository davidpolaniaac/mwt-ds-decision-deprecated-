enabling BFGS based optimization **without** curvature calculation
Num weight bits = 20
learning rate = 0.5
initial_t = 1
power_t = 0.5
decay_learning_rate = 1
using l2 regularization = 1
m = 7
Allocated 72M for weights and mem
## avg. loss 	der. mag. 	d. m. cond.	 wolfe1    	wolfe2    	mix fraction	curvature 	dir. magnitude	step size 	time      
creating cache_file = train-sets/rcv1_small.dat.cache
Reading from train-sets/rcv1_small.dat
num sources = 1
 1 6.931472e-01	1.247013e-02	1.093747e+02	          	          	          	1.652544e+02	1.072661e+06	6.618564e-01	0.289
 3 2.496379e+01	9.448176e-02	5.849782e+01	 -0.335275 	-0.667864 	          	          	(revise x 0.5)	3.309282e-01	0.321
 4 6.688902e+00	7.916274e-02	2.138882e+01	 -0.165651 	-0.341257 	          	          	(revise x 0.5)	1.654641e-01	0.362
 5 2.025952e+00	5.333706e-02	9.612557e+00	 -0.073645 	-0.172136 	          	          	(revise x 0.5)	8.273205e-02	0.403
 6 8.860910e-01	2.270515e-02	4.214893e+00	 -0.021323 	-0.077380 	          	          	(revise x 0.5)	4.136603e-02	0.443
 7 6.578700e-01	5.668358e-03	2.422930e+00	 0.007797  	-0.022375 	          	          	(revise x 0.5)	2.068301e-02	0.485
 8 6.409029e-01	1.090401e-03	2.538482e+00	 0.023094  	0.007617  	          	          	5.593078e+00	1.000000e+00	0.595     
 9 5.915731e-01	8.417667e-04	1.984715e+00	 0.939814  	0.880475  	          	          	4.497635e+02	1.000000e+00	0.733     
10 3.619765e-01	1.536667e-04	2.219506e-01	 0.590336  	0.255459  	          	          	1.230659e+02	1.000000e+00	0.896     
11 3.481746e-01	5.063765e-03	6.800128e-01	 0.218977  	-0.427065 	          	          	2.743107e+01	1.000000e+00	1.078     
12 3.232802e-01	2.902376e-04	8.337535e-02	 0.597309  	0.217820  	          	          	3.598937e+00	1.000000e+00	1.283     
13 3.198322e-01	1.480065e-05	4.911858e-02	 0.738430  	0.480967  	          	          	8.913005e+00	1.000000e+00	1.514     
14 3.159695e-01	5.730583e-05	4.996126e-02	 0.762546  	0.528461  	          	          	2.429691e+01	1.000000e+00	1.773     
15 3.108274e-01	1.807561e-04	5.458046e-02	 0.726800  	0.455732  	          	          	1.030238e+02	1.000000e+00	2.041     
16 2.980001e-01	3.906264e-05	1.068944e-02	 0.692397  	0.384367  	          	          	5.095499e+01	1.000000e+00	2.294     
17 2.948572e-01	6.621593e-08	1.792561e-04	 0.516694  	0.031775  	          	          	3.948198e-01	1.000000e+00	2.545     
18 2.947937e-01	1.091626e-07	8.738563e-05	 0.595950  	0.192321  	          	          	2.461794e-01	1.000000e+00	2.796     

finished run
number of examples = 200000
weighted example sum = 2e+05
weighted label sum = -1.272e+04
average loss = 0.4485
best constant = -0.06361
total feature number = 15587880
19 2.947603e-01	1.980417e-08	3.041856e-05	 0.619564  	0.238334  	          	          	1.564171e-01	1.000000e+00	3.075     

