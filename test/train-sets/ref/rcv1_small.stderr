enabling BFGS based optimization **without** curvature calculation
Num weight bits = 20
learning rate = 0.5
initial_t = 1
power_t = 0.5
decay_learning_rate = 1
using l2 regularization = 1
m = 7
Allocated 72M for weights and mem
## avg. loss 	der. mag. 	d. m. cond.	 wolfe1    	wolfe2    	mix fraction	curvature 	dir. magnitude	step size 	time      
creating cache_file = train-sets/rcv1_small.dat.cache
Reading from train-sets/rcv1_small.dat
num sources = 1
 1 6.931472e-01	1.247013e-02	1.093747e+02	          	          	          	1.652544e+02	1.072661e+06	6.618565e-01	0.340
 3 2.496380e+01	9.448176e-02	5.849783e+01	 -0.335275 	-0.667864 	          	          	(revise x 0.5)	3.309282e-01	0.389
 4 6.688904e+00	7.916274e-02	2.138882e+01	 -0.165651 	-0.341257 	          	          	(revise x 0.5)	1.654641e-01	0.467
 5 2.025952e+00	5.333705e-02	9.612558e+00	 -0.073645 	-0.172136 	          	          	(revise x 0.5)	8.273206e-02	0.549
 6 8.860911e-01	2.270515e-02	4.214893e+00	 -0.021323 	-0.077380 	          	          	(revise x 0.5)	4.136603e-02	0.615
 7 6.578700e-01	5.668351e-03	2.422930e+00	 0.007797  	-0.022375 	          	          	(revise x 0.5)	2.068301e-02	0.683
 8 6.409030e-01	1.090398e-03	2.538483e+00	 0.023094  	0.007617  	          	          	5.593080e+00	1.000000e+00	0.866     
 9 5.915731e-01	8.417664e-04	1.984715e+00	 0.939813  	0.880475  	          	          	4.497636e+02	1.000000e+00	1.079     
10 3.619765e-01	1.536772e-04	2.219526e-01	 0.590336  	0.255459  	          	          	1.230677e+02	1.000000e+00	1.332     
11 3.481754e-01	5.063975e-03	6.800376e-01	 0.218961  	-0.427092 	          	          	2.743005e+01	1.000000e+00	1.629     
12 3.232804e-01	2.902589e-04	8.337796e-02	 0.597312  	0.217825  	          	          	3.599038e+00	1.000000e+00	1.957     
13 3.198323e-01	1.480102e-05	4.911876e-02	 0.738423  	0.480953  	          	          	8.912497e+00	1.000000e+00	2.319     
14 3.159697e-01	5.730242e-05	4.996120e-02	 0.762549  	0.528468  	          	          	2.429698e+01	1.000000e+00	2.748     
15 3.108275e-01	1.807548e-04	5.458054e-02	 0.726800  	0.455732  	          	          	1.030220e+02	1.000000e+00	3.133     
16 2.980002e-01	3.906509e-05	1.069003e-02	 0.692401  	0.384374  	          	          	5.095741e+01	1.000000e+00	3.498     
17 2.948572e-01	6.624650e-08	1.792463e-04	 0.516694  	0.031777  	          	          	3.947939e-01	1.000000e+00	3.945     
18 2.947937e-01	1.091701e-07	8.735795e-05	 0.596024  	0.192459  	          	          	2.462853e-01	1.000000e+00	4.304     

finished run
number of examples = 200000
weighted example sum = 2e+05
weighted label sum = -1.272e+04
average loss = 0.4485
best constant = -0.06361
total feature number = 15587880
19 2.947603e-01	1.979209e-08	3.042067e-05	 0.619453  	0.238180  	          	          	1.564096e-01	1.000000e+00	4.705     

