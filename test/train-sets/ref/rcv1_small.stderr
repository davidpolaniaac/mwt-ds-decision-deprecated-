enabling BFGS based optimization **without** curvature calculation
Num weight bits = 20
learning rate = 0.5
initial_t = 1
power_t = 0.5
decay_learning_rate = 1
using l2 regularization = 1
m = 7
Allocated 72M for weights and mem
## avg. loss 	der. mag. 	d. m. cond.	 wolfe1    	wolfe2    	mix fraction	curvature 	dir. magnitude	step size 	time      
creating cache_file = train-sets/rcv1_small.dat.cache
Reading from train-sets/rcv1_small.dat
num sources = 1
 1 0.69315   	0.01247   	109.37472 	          	          	          	165.25444 	1072660.62500	0.66186   	0.295
 3 24.96380  	0.09448   	58.49783  	 -0.335275 	-0.667864 	          	          	(revise x 0.5)	0.33093   	0.342
 4 6.68890   	0.07916   	21.38882  	 -0.165651 	-0.341257 	          	          	(revise x 0.5)	0.16546   	0.403
 5 2.02595   	0.05334   	9.61256   	 -0.073645 	-0.172136 	          	          	(revise x 0.5)	0.08273   	0.463
 6 0.88609   	0.02271   	4.21489   	 -0.021323 	-0.077380 	          	          	(revise x 0.5)	0.04137   	0.523
 7 0.65787   	0.00567   	2.42293   	 0.007797  	-0.022375 	          	          	(revise x 0.5)	0.02068   	0.583
 8 0.64090   	0.00109   	2.53848   	 0.023094  	0.007617  	          	          	5.59308   	1.00000   	0.751     
 9 0.59157   	0.00084   	1.98472   	 0.939814  	0.880475  	          	          	449.76361 	1.00000   	0.944     
10 0.36198   	0.00015   	0.22195   	 0.590336  	0.255459  	          	          	123.06779 	1.00000   	1.169     
11 0.34818   	0.00506   	0.68004   	 0.218960  	-0.427093 	          	          	27.43001  	1.00000   	1.426     
12 0.32328   	0.00029   	0.08338   	 0.597312  	0.217825  	          	          	3.59904   	1.00000   	1.717     
13 0.31983   	0.00001   	0.04912   	 0.738423  	0.480952  	          	          	8.91247   	1.00000   	2.037     
14 0.31597   	0.00006   	0.04996   	 0.762549  	0.528469  	          	          	24.29697  	1.00000   	2.391     
15 0.31083   	0.00018   	0.05458   	 0.726800  	0.455732  	          	          	103.02180 	1.00000   	2.746     
16 0.29800   	0.00004   	0.01069   	 0.692401  	0.384374  	          	          	50.95761  	1.00000   	3.099     
17 0.29486   	0.00000   	0.00018   	 0.516695  	0.031777  	          	          	0.39479   	1.00000   	3.455     
18 0.29479   	0.00000   	0.00009   	 0.596026  	0.192467  	          	          	0.24629   	1.00000   	3.810     

finished run
number of examples = 200000
weighted example sum = 2e+05
weighted label sum = -1.272e+04
average loss = 0.4485
best constant = -0.06361
total feature number = 15587880
19 0.29476   	0.00000   	0.00003   	 0.619462  	0.238168  	          	          	0.15641   	1.00000   	4.188     

