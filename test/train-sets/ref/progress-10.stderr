Num weight bits = 18
learning rate = 0.5
initial_t = 0
power_t = 0.5
using no cache
Reading datafile = train-sets/0001.dat
num sources = 1
average    since         example     example  current  current  current
loss       last          counter      weight    label  predict features
0.346304   0.346304            3         3.0   0.0000   0.1490       57
0.214802   0.175351           13        13.0   0.0000   0.1981       87
0.261079   0.321238           23        23.0   0.0000   0.3577      137
0.231331   0.162911           33        33.0   0.0000   0.1890      224
0.230834   0.229196           43        43.0   0.0000   0.4018      142
0.231985   0.236935           53        53.0   0.0000   0.3871      208
0.236993   0.263533           63        63.0   1.0000   0.3928       17
0.233871   0.214201           73        73.0   1.0000   0.3056       54
0.222402   0.138676           83        83.0   1.0000   0.6377       47
0.216129   0.164069           93        93.0   1.0000   0.9731       58
0.220717   0.263382          103       103.0   0.0000   0.4643       79
0.217901   0.188896          113       113.0   0.0000   0.4878       99
0.222734   0.277346          123       123.0   0.0000   0.4768       36
0.218288   0.163611          133       133.0   0.0000   0.2414       14
0.213985   0.156743          143       143.0   1.0000   0.8205       19
0.210730   0.164190          153       153.0   1.0000   0.6932       66
0.209006   0.182634          163       163.0   0.0000   0.4407      196
0.203219   0.108890          173       173.0   0.0000   0.2820       91
0.201161   0.165562          183       183.0   1.0000   0.5772       27
0.196804   0.117062          193       193.0   0.0000   0.2992       60

finished run
number of examples per pass = 200
passes used = 1
weighted example sum = 200
weighted label sum = 91
average loss = 0.19509
best constant = 0.455
best constant's loss = 0.247975
total feature number = 15482
