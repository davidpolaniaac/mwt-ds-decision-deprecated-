Num weight bits = 18
learning rate = 0.5
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
creating cache_file = train-sets/big-constant.dat.cache
Reading datafile = train-sets/big-constant.dat
num sources = 1
average    since         example     example  current  current  current
loss       last          counter      weight    label  predict features
1.636128   1.636128            3         3.0 997.5570 999.6422       23
4.680823   7.725518            6         6.0 996.7601 1000.1057       23
3.590172   2.281390           11        11.0 1002.7795 1000.5263       23
2.826036   2.061901           22        22.0 999.4972 999.6194       23
2.307562   1.789087           44        44.0 1001.0120 999.6308       23
1.827814   1.336910           87        87.0 1000.8730 999.4970       23
1.521127   1.214440          174       174.0 997.5120 998.7500       23
1.137631   0.754134          348       348.0 999.2150 999.6852       23
0.761520   0.385410          696       696.0 999.5161 999.8427       23
0.455825   0.150129         1392      1392.0 999.5844 999.1084       23
0.248140   0.040456         2784      2784.0 996.4570 996.5460       23
0.127489   0.006838         5568      5568.0 1000.3337 1000.3330       23

finished run
number of examples per pass = 100
passes used = 100
weighted example sum = 10000
weighted label sum = 9.99896e+06
average loss = 0.0712402
best constant = 999.896
total feature number = 230000
