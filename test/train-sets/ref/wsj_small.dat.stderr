creating cache_file = train-sets/wsj_small.dat.gz.cache
Reading from train-sets/wsj_small.dat.gz
num sources = 1
Num weight bits = 18
learning rate = 10
initial_t = 1
power_t = 0.5
decay_learning_rate = 1
average    since      sequence  example            current label      current predicted  current   cur   cur
loss       last        counter   weight          sequence prefix        sequence prefix features  pass   pol
0.810811   0.810811          1     37.0   [  1   2   3   1   4 ] [  1   1   1   1   1 ]     1654     0     0
0.750000   0.666667          2     64.0   [ 11   2   3  11  11 ] [  1   2  11  12   9 ]     1194     0     0
0.698925   0.586207          3     93.0   [ 14  10  13   9   1 ] [ 11  11  11  15   9 ]     1286     0     0
0.775194   0.972222          4    129.0   [  3   4   6   3   1 ] [ 11  11  11  11  11 ]     1608     0     0
0.756250   0.677419          5    160.0   [ 19   3  10   2   1 ] [ 14  10   1   2   1 ]     1378     0     0
0.724490   0.583333          6    196.0   [ 19   2  22   4   3 ] [ 19   2  11  11  11 ]     1608     0     0
0.744681   0.846154          7    235.0   [ 10   2   3   1  10 ] [ 19   2  11  11  11 ]     1746     0     0
0.705382   0.627119         12    353.0   [  5  12  11  11  21 ] [ 11  12   9   1   2 ]     1102     0     0
0.575284   0.444444         25    704.0   [ 10  13  22   4   9 ] [ 10  13   3   9   1 ]     1148     0     0
0.482370   0.390756         57   1418.0   [ 19   1   4   6  36 ] [ 19   3   4   6   5 ]     2252     0     0
0.309345   0.130909        110   2793.0   [  9   1  10  21   2 ] [  9   1  10  21   2 ]     1792     1     1

finished run
number of examples = 156
weighted example sum = 3864
weighted label sum = 0
average loss = 0.2345
best constant = -0.0002589
total feature number = 170256
