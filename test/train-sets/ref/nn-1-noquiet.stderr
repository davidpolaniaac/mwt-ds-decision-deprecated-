using l2 regularization = 0.0003
Num weight bits = 18
learning rate = 0.7
initial_t = 0
power_t = 0.5
using no cache
Reading datafile = train-sets/0002.dat
num sources = 1
average    since         example     example  current  current  current
loss       last          counter      weight    label  predict features
0.097615   0.097615            1         1.0   0.5211   0.2087       15
0.049183   0.000751            2         2.0   0.5353   0.5080       15
0.025087   0.000992            4         4.0   0.5854   0.5529       15
0.023376   0.021664            8         8.0   0.5575   0.6405       15
0.012003   0.000631           16        16.0   0.5878   0.5655       15
0.008726   0.005448           32        32.0   0.6038   0.5971       15
0.005598   0.002470           64        64.0   0.5683   0.5217       15
0.004921   0.004245          128       128.0   0.5351   0.5740       15
0.003856   0.002790          256       256.0   0.5385   0.5767       15
0.003759   0.003662          512       512.0   0.5053   0.5441       15
0.004285   0.004811         1024      1024.0   0.5750   0.6570       15
0.004549   0.004813         2048      2048.0   0.5204   0.8673       15
0.003498   0.002447         4096      4096.0   0.5042   1.0000       15
0.004751   0.006005         8192      8192.0   0.4967   0.7146       15
0.004894   0.005038        16384     16384.0   0.5011   0.5133       15
0.004919   0.004944        32768     32768.0   0.3915   1.0000       15
0.005185   0.005450        65536     65536.0   0.5043   0.5633       15

finished run
number of examples per pass = 74746
passes used = 1
weighted example sum = 69521
weighted label sum = 35113.3
average loss = 0.00508264
best constant = 0.505074
best constant's loss = 0.249974
total feature number = 1119986
