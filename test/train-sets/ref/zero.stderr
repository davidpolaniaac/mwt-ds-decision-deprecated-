enabling BFGS based optimization **without** curvature calculation
Num weight bits = 20
learning rate = 0.5
initial_t = 1
power_t = 0.5
decay_learning_rate = 1
using l2 regularization = 1
m = 7
Allocated 72M for weights and mem
## avg. loss 	der. mag. 	d. m. cond.	 wolfe1    	wolfe2    	mix fraction	curvature 	dir. magnitude	step size 	time      
creating cache_file = train-sets/zero.dat.cache
Reading from train-sets/zero.dat
num sources = 1
 1 0.00000   	41943.04000	209549.29466	          	          	          	223389.60340	1047595.31250	0.93804   	0.120
 3 92180.70573	36872.28229	184355.04136	 -0.468954 	-0.937909 	          	          	(revise x 0.5)	0.46902   	0.144
 4 23045.17643	9218.07057	46088.76034	 -0.234477 	-0.468954 	          	          	(revise x 0.5)	0.23451   	0.182

finished run
number of examples = 25
weighted example sum = 25
weighted label sum = 0
average loss = 0
best constant = -0.04167
total feature number = 15005
 5 5761.29411	2304.51764	11522.19008	 -0.117239 	-0.234477 	          	          	(revise x 0.5)	0.11726   	0.220

