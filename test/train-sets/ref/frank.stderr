enabling BFGS based optimization **without** curvature calculation
creating cache_file = train-sets/frank.dat.cache
Reading from train-sets/frank.dat
num sources = 1
Num weight bits = 20
learning rate = 10
initial_t = 1
power_t = 0.5
decay_learning_rate = 1
using l2 regularization
m = 7
Allocated 72M for weights and mem
## avg. loss 	der. mag. 	d. m. cond.	 wolfe1    	wolfe2    	mix fraction	curvature 	dir. magnitude	step size 	time      
 1 3.313292e+12	9.499654e+14	1.905397e+13	          	          	          	6.440241e+13	2.698544e+12	2.958580e-01	0.344
 3 4.946559e+11	1.587410e+13	2.431758e+11	 0.500000  	0.000001  	          	          	4.134237e+08	1.000000e+00	0.505     
 4 4.262257e+11	1.360521e+13	2.087762e+11	 0.963277  	0.926554  	          	          	9.978069e+10	1.000000e+00	0.709     
 5 5.509102e+09	1.685101e+10	8.165170e+08	 0.502725  	0.005668  	          	          	4.928417e+09	1.000000e+00	0.950     
 6 2.722920e+09	9.129883e+09	4.115953e+08	 0.848425  	0.697673  	          	          	2.678168e+10	1.000000e+00	1.222     
 7 5.131463e+06	6.501375e+06	9.870512e+04	 0.499530  	-0.000676 	          	          	2.672022e+05	1.000000e+00	1.528     
 8 5.963145e+06	3.409090e+07	5.131574e+05	 -0.644867 	-2.289865 	          	          	(revise x 0.5)	5.000000e-01	1.569
 9 5.016942e+06	2.720016e+06	4.109481e+04	 0.177594  	-0.644885 	          	          	1.319267e+04	1.000000e+00	1.919     
10 4.934764e+06	8.327155e+00	1.628377e-01	 0.499929  	-0.000128 	          	          	3.497378e-03	1.000000e+00	2.293     


finished run
number of examples = 100000
weighted example sum = 1e+05
weighted label sum = -1.577e+11
average loss = 7.556e+11
best constant = -1.577e+06
total feature number = 400000
