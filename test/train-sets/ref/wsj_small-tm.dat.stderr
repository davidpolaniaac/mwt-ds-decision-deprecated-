using cache_file = train-sets/wsj_small.dat.gz.cache
ignoring text input in favor of cache input
num sources = 1
Num weight bits = 18
learning rate = 10
initial_t = 1
power_t = 0.5
average    since      sequence  example            current label      current predicted  current   cur   cur    predic.   examples
loss       last        counter   weight          sequence prefix        sequence prefix features  pass   pol       made     gener.
0.810811   0.810811          1     37.0   [  1   2   3   1   4 ] [  1   1   1   1   1 ]     1654     0     0         37          0
0.781250   0.740741          2     64.0   [ 11   2   3  11  11 ] [ 11  26   9  11  26 ]     1194     0     0        705         37
0.731183   0.620690          3     93.0   [ 14  10  13   9   1 ] [ 11  15  16   1   1 ]     1286     0     0       1105         64
0.720930   0.694444          4    129.0   [  3   4   6   3   1 ] [ 11  11   2   3  11 ]     1608     0     0       1494         93
0.706250   0.645161          5    160.0   [ 19   3  10   2   1 ] [  2   3   1   2   1 ]     1378     0     0       2170        129
0.678571   0.555556          6    196.0   [ 19   2  22   4   3 ] [ 11   2  11  11  11 ]     1608     0     0       2462        160
0.676596   0.666667          7    235.0   [ 10   2   3   1  10 ] [  1   2  11   1   1 ]     1746     0     0       3061        196
0.614731   0.491525         12    353.0   [  5  12  11  11  21 ] [ 11  12   9   1  21 ]     1102     0     0       5473        328
0.482955   0.350427         25    704.0   [ 10  13  22   4   9 ] [ 10   2   1   4   1 ]     1148     0     0      12574        678
0.398449   0.315126         57   1418.0   [ 19   1   4   6  36 ] [ 19   1   4   6   5 ]     2252     0     0      25497       1368

finished run
number of examples = 78
weighted example sum = 1932
weighted label sum = 0
average loss = 0.367
best constant = -0.0005179
total feature number = 85128
