Num weight bits = 18
learning rate = 4
initial_t = 1
power_t = 0.5
predictions = 0002c.predict
only testing
using no cache
Reading from train-sets/0002.dat
num sources = 1
average    since         example     example  current  current  current
loss       last          counter      weight    label  predict features
0.002715   0.002715            3         3.0   0.5498   0.5067       15
0.024634   0.046554            6         6.0   0.2681   0.5730       15
0.025404   0.026327           11        11.0   0.4315   0.5330       15
0.038383   0.051363           22        22.0   0.5519   0.4564       15
0.026522   0.014662           44        44.0   0.5514   0.5145       15
0.023545   0.020499           87        87.0   0.5140   0.5638       15
0.023022   0.022498          174       174.0   0.5596   0.4431       15
0.019465   0.015909          348       348.0   0.5475   0.4272       15
0.017449   0.015432          696       696.0   0.3421   0.7915       15
0.018162   0.018875         1392      1392.0   0.4996   0.5059       15
0.015313   0.012465         2784      2784.0   0.5090   0.4542       15
0.012718   0.010122         5568      5568.0   0.6413   0.6955       15
0.011434   0.010150        11135     11135.0   0.3869   0.3792       15
0.010238   0.009043        22269     22269.0   0.5063   0.4637       15
0.014878   0.019518        44537     44537.0   0.4905   0.4432       15

finished run
number of examples = 74746
weighted example sum = 6.952e+04
weighted label sum = 3.511e+04
average loss = 0.01343
best constant = 0.5051
best constant's loss = 0.25
total feature number = 1121190
