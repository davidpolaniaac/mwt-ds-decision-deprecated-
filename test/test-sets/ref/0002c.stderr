Num weight bits = 18
learning rate = 4
initial_t = 1
power_t = 0.5
predictions = 0002c.predict
only testing
using no cache
Reading from train-sets/0002.dat
num sources = 1
average    since         example     example  current  current  current
loss       last          counter      weight    label  predict features
0.003357   0.003357            3         3.0   0.5498   0.5194      184
0.025001   0.046645            6         6.0   0.2681   0.6030      184
0.021822   0.018007           11        11.0   0.4315   0.5632      184
0.037093   0.052364           22        22.0   0.5519   0.5173      184
0.022365   0.007637           44        44.0   0.5514   0.5418      184
0.021048   0.019700           87        87.0   0.5140   0.5620      184
0.017350   0.013653          174       174.0   0.5596   0.4952      184
0.015064   0.012777          348       348.0   0.5475   0.4873      184
0.014542   0.014021          696       696.0   0.3421   0.7862      184
0.014173   0.013804         1392      1392.0   0.4996   0.5262      184
0.013207   0.012240         2784      2784.0   0.5090   0.4033      184
0.012721   0.012235         5568      5568.0   0.6413   0.8874      184
0.011278   0.009836        11135     11135.0   0.3869   0.4603      184
0.012195   0.013111        22269     22269.0   0.5063   0.4577      184
0.018275   0.024355        44537     44537.0   0.4905   0.5400      184

finished run
number of examples = 74746
weighted example sum = 6.952e+04
weighted label sum = 3.511e+04
average loss = 0.01667
best constant = 0.5051
best constant's loss = 0.25
total feature number = 13753264
