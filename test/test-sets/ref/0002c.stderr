using no cache
Reading from train-sets/0002.dat
num sources = 1
Num weight bits = 18
learning rate = 10
initial_t = 1
power_t = 0.5
predictions = 0002c.predict
only testing
average    since       example  example    current  current  current
loss       last        counter   weight      label  predict features
0.001747   0.001747          3      3.0     0.5498   0.5289      184
0.027184   0.052621          6      6.0     0.2681   0.6451      184
0.024323   0.020890         11     11.0     0.4315   0.5887      184
0.034173   0.044024         22     22.0     0.5519   0.5049      184
0.021027   0.007881         44     44.0     0.5514   0.5307      184
0.020252   0.019460         87     87.0     0.5140   0.4891      184
0.015007   0.009762        174    174.0     0.5596   0.5018      184
0.012415   0.009822        348    348.0     0.5475   0.4353      184
0.011138   0.009862        696    696.0     0.3421   0.7196      184
0.011188   0.011238       1392   1392.0     0.4996   0.4918      184
0.009234   0.007279       2784   2784.0     0.5090   0.4131      184
0.008574   0.007915       5568   5568.0     0.6413   0.6159      184
0.007983   0.007391      11135  11135.0     0.3869   0.4991      184
0.008372   0.008762      22269  22269.0     0.5063   0.4469      184
0.011464   0.014555      44537  44537.0     0.4905   0.4894      184

finished run
number of examples = 74746
weighted example sum = 6.952e+04
weighted label sum = 3.511e+04
average loss = 0.00979
best constant = 0.5051
best constant's loss = 0.25
total feature number = 13753264
