Num weight bits = 18
learning rate = 10
initial_t = 1
power_t = 0.5
predictions = 0002c.predict
only testing
using no cache
Reading from train-sets/0002.dat
num sources = 1
average    since         example     example  current  current  current
loss       last          counter      weight    label  predict features
0.003393   0.003393            3         3.0   0.5498   0.5256       15
0.019675   0.035958            6         6.0   0.2681   0.5213       15
0.021545   0.023789           11        11.0   0.4315   0.5435       15
0.030151   0.038757           22        22.0   0.5519   0.4713       15
0.021701   0.013252           44        44.0   0.5514   0.5224       15
0.017492   0.013185           87        87.0   0.5140   0.5879       15
0.015067   0.012641          174       174.0   0.5596   0.4714       15
0.013692   0.012317          348       348.0   0.5475   0.4640       15
0.012417   0.011141          696       696.0   0.3421   0.6418       15
0.013141   0.013866         1392      1392.0   0.4996   0.5202       15
0.012969   0.012796         2784      2784.0   0.5090   0.3784       15
0.011588   0.010206         5568      5568.0   0.6413   0.7439       15
0.010168   0.008749        11135     11135.0   0.3869   0.4506       15
0.010673   0.011177        22269     22269.0   0.5063   0.4666       15
0.016128   0.021584        44537     44537.0   0.4905   0.5041       15

finished run
number of examples = 74746
weighted example sum = 6.952e+04
weighted label sum = 3.511e+04
average loss = 0.01481
best constant = 0.5051
best constant's loss = 0.25
total feature number = 1121190
